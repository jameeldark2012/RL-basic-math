{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888e3a0e",
   "metadata": {},
   "source": [
    "# 1.RL Sample Average Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf2179e-8cfb-46ff-911f-9adf41c964c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x280fe39d930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch   \n",
    "import random\n",
    "torch.manual_seed(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e8267a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d4970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44223591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 5, 3, 1, 3, 2, 5, 2, 5, 5, 1, 4, 5, 4, 3, 3, 5, 2, 2, 5, 5, 1, 5,\n",
       "        2, 4, 5, 5, 2, 5, 1, 2, 5, 4, 1, 1, 5, 4, 2, 3, 2, 1, 2, 2, 1, 2, 1, 2,\n",
       "        1, 1, 4, 5, 3, 1, 5, 5, 5, 5, 1, 2, 1, 3, 2, 1, 2, 4, 3, 2, 1, 2, 3, 4,\n",
       "        2, 5, 5, 2, 1, 4, 5, 4, 3, 5, 3, 2, 5, 1, 1, 1, 5, 1, 1, 2, 1, 5, 3, 3,\n",
       "        2, 4, 3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = torch.randint(1 , 6 , (BATCH_SIZE,) , device=device)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b6b909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d479f7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8344, 0.7001, 0.5361, 0.8883, 0.0220, 0.7938, 0.7543, 0.6514, 0.9609,\n",
       "        0.8294, 0.3537, 0.8665, 0.2563, 0.5770, 0.0433, 0.0837, 0.6263, 0.5618,\n",
       "        0.6589, 0.4808, 0.6172, 0.5818, 0.0753, 0.0792, 0.0765, 0.2946, 0.1289,\n",
       "        0.1717, 0.3002, 0.7698, 0.4647, 0.7871, 0.2272, 0.2296, 0.0723, 0.7259,\n",
       "        0.2371, 0.4740, 0.7083, 0.0154, 0.9985, 0.1717, 0.7336, 0.3570, 0.6377,\n",
       "        0.0669, 0.0436, 0.3026, 0.4370, 0.5717, 0.2362, 0.5862, 0.7265, 0.2540,\n",
       "        0.1194, 0.1819, 0.5693, 0.9899, 0.5516, 0.3524, 0.3854, 0.4761, 0.2965,\n",
       "        0.8352, 0.4146, 0.6716, 0.3455, 0.8389, 0.5062, 0.7360, 0.8370, 0.6655,\n",
       "        0.5389, 0.7631, 0.3010, 0.3146, 0.1132, 0.4187, 0.3295, 0.4154, 0.9915,\n",
       "        0.2093, 0.6067, 0.3841, 0.7249, 0.7976, 0.1511, 0.6984, 0.8953, 0.1742,\n",
       "        0.8720, 0.3984, 0.4770, 0.8990, 0.9964, 0.0501, 0.4891, 0.5103, 0.9123,\n",
       "        0.3186], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = torch.rand((BATCH_SIZE,) , device=device)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5c52e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c9d6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000, 0.8344],\n",
       "        [3.0000, 0.7001],\n",
       "        [5.0000, 0.5361],\n",
       "        [3.0000, 0.8883],\n",
       "        [1.0000, 0.0220],\n",
       "        [3.0000, 0.7938],\n",
       "        [2.0000, 0.7543],\n",
       "        [5.0000, 0.6514],\n",
       "        [2.0000, 0.9609],\n",
       "        [5.0000, 0.8294],\n",
       "        [5.0000, 0.3537],\n",
       "        [1.0000, 0.8665],\n",
       "        [4.0000, 0.2563],\n",
       "        [5.0000, 0.5770],\n",
       "        [4.0000, 0.0433],\n",
       "        [3.0000, 0.0837],\n",
       "        [3.0000, 0.6263],\n",
       "        [5.0000, 0.5618],\n",
       "        [2.0000, 0.6589],\n",
       "        [2.0000, 0.4808],\n",
       "        [5.0000, 0.6172],\n",
       "        [5.0000, 0.5818],\n",
       "        [1.0000, 0.0753],\n",
       "        [5.0000, 0.0792],\n",
       "        [2.0000, 0.0765],\n",
       "        [4.0000, 0.2946],\n",
       "        [5.0000, 0.1289],\n",
       "        [5.0000, 0.1717],\n",
       "        [2.0000, 0.3002],\n",
       "        [5.0000, 0.7698],\n",
       "        [1.0000, 0.4647],\n",
       "        [2.0000, 0.7871],\n",
       "        [5.0000, 0.2272],\n",
       "        [4.0000, 0.2296],\n",
       "        [1.0000, 0.0723],\n",
       "        [1.0000, 0.7259],\n",
       "        [5.0000, 0.2371],\n",
       "        [4.0000, 0.4740],\n",
       "        [2.0000, 0.7083],\n",
       "        [3.0000, 0.0154],\n",
       "        [2.0000, 0.9985],\n",
       "        [1.0000, 0.1717],\n",
       "        [2.0000, 0.7336],\n",
       "        [2.0000, 0.3570],\n",
       "        [1.0000, 0.6377],\n",
       "        [2.0000, 0.0669],\n",
       "        [1.0000, 0.0436],\n",
       "        [2.0000, 0.3026],\n",
       "        [1.0000, 0.4370],\n",
       "        [1.0000, 0.5717],\n",
       "        [4.0000, 0.2362],\n",
       "        [5.0000, 0.5862],\n",
       "        [3.0000, 0.7265],\n",
       "        [1.0000, 0.2540],\n",
       "        [5.0000, 0.1194],\n",
       "        [5.0000, 0.1819],\n",
       "        [5.0000, 0.5693],\n",
       "        [5.0000, 0.9899],\n",
       "        [1.0000, 0.5516],\n",
       "        [2.0000, 0.3524],\n",
       "        [1.0000, 0.3854],\n",
       "        [3.0000, 0.4761],\n",
       "        [2.0000, 0.2965],\n",
       "        [1.0000, 0.8352],\n",
       "        [2.0000, 0.4146],\n",
       "        [4.0000, 0.6716],\n",
       "        [3.0000, 0.3455],\n",
       "        [2.0000, 0.8389],\n",
       "        [1.0000, 0.5062],\n",
       "        [2.0000, 0.7360],\n",
       "        [3.0000, 0.8370],\n",
       "        [4.0000, 0.6655],\n",
       "        [2.0000, 0.5389],\n",
       "        [5.0000, 0.7631],\n",
       "        [5.0000, 0.3010],\n",
       "        [2.0000, 0.3146],\n",
       "        [1.0000, 0.1132],\n",
       "        [4.0000, 0.4187],\n",
       "        [5.0000, 0.3295],\n",
       "        [4.0000, 0.4154],\n",
       "        [3.0000, 0.9915],\n",
       "        [5.0000, 0.2093],\n",
       "        [3.0000, 0.6067],\n",
       "        [2.0000, 0.3841],\n",
       "        [5.0000, 0.7249],\n",
       "        [1.0000, 0.7976],\n",
       "        [1.0000, 0.1511],\n",
       "        [1.0000, 0.6984],\n",
       "        [5.0000, 0.8953],\n",
       "        [1.0000, 0.1742],\n",
       "        [1.0000, 0.8720],\n",
       "        [2.0000, 0.3984],\n",
       "        [1.0000, 0.4770],\n",
       "        [5.0000, 0.8990],\n",
       "        [3.0000, 0.9964],\n",
       "        [3.0000, 0.0501],\n",
       "        [2.0000, 0.4891],\n",
       "        [4.0000, 0.5103],\n",
       "        [3.0000, 0.9123],\n",
       "        [3.0000, 0.3186]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_rewards = torch.stack(  (actions , rewards) , dim=1)\n",
    "actions_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a8d20d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000, 0.5361],\n",
       "        [5.0000, 0.6514],\n",
       "        [5.0000, 0.8294],\n",
       "        [5.0000, 0.3537],\n",
       "        [5.0000, 0.5770],\n",
       "        [5.0000, 0.5618],\n",
       "        [5.0000, 0.6172],\n",
       "        [5.0000, 0.5818],\n",
       "        [5.0000, 0.0792],\n",
       "        [5.0000, 0.1289],\n",
       "        [5.0000, 0.1717],\n",
       "        [5.0000, 0.7698],\n",
       "        [5.0000, 0.2272],\n",
       "        [5.0000, 0.2371],\n",
       "        [5.0000, 0.5862],\n",
       "        [5.0000, 0.1194],\n",
       "        [5.0000, 0.1819],\n",
       "        [5.0000, 0.5693],\n",
       "        [5.0000, 0.9899],\n",
       "        [5.0000, 0.7631],\n",
       "        [5.0000, 0.3010],\n",
       "        [5.0000, 0.3295],\n",
       "        [5.0000, 0.2093],\n",
       "        [5.0000, 0.7249],\n",
       "        [5.0000, 0.8953],\n",
       "        [5.0000, 0.8990]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_rewards[actions_rewards[: , 0] == 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823e861b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 0.5195, 0.6002, 0.3832, 0.4958], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rewards = torch.stack([rewards[actions == i].mean() for i in actions.unique()])\n",
    "mean_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185846c5",
   "metadata": {},
   "source": [
    "### 1.1 Increamental Update Rule for non stationary enviroments : <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>Q</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Q</mi><mi>n</mi></msub><mo>+</mo><mi>α</mi><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>n</mi></msub><mo>−</mo><msub><mi>Q</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Q_{n+1} = Q_n + \\alpha (R_n - Q_n) \n",
    "</annotation></semantics></math>\n",
    "\n",
    "( Q_n ) is the current estimate of the action value.\n",
    "( R_n ) is the reward received after taking the action.\n",
    "( alpha ) is the step size parameter, which determines how much weight to give to the new reward compared to the old estimate2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36802e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(qn , a , rn ):\n",
    "\n",
    "    try:\n",
    "        for i in range(len(rn)):\n",
    "            qn = qn + a * (rn[i] - qn)\n",
    "            print(qn)\n",
    "        return qn\n",
    "\n",
    "    except:\n",
    "        qn_1 = qn + a * (rn - qn)\n",
    "        return qn_1\n",
    "\n",
    "    \n",
    "\n",
    "rn = torch.tensor(0.9 , device=device )\n",
    "qn = mean_rewards[0]\n",
    "a = torch.tensor(1/len(actions_rewards) , device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2631b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4306, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c436707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4353, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn_1 = update(qn ,a , rn)\n",
    "qn_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9151fbe",
   "metadata": {},
   "source": [
    "### Experiement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5df183d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn = torch.tensor(5 , device=device)\n",
    "a = torch.tensor(0.5 , device=device)\n",
    "rn = torch.randint(0 , 2 , (100,) , device=device)\n",
    "rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88cd9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The q0 value got updated based on our rewards , missing aroudn with qn initial value affects the process naking the agent more optimistic or pessemstic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02ee80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., device='cuda:0')\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(0.5000, device='cuda:0')\n",
      "tensor(0.7500, device='cuda:0')\n",
      "tensor(0.8750, device='cuda:0')\n",
      "tensor(0.4375, device='cuda:0')\n",
      "tensor(0.2188, device='cuda:0')\n",
      "tensor(0.1094, device='cuda:0')\n",
      "tensor(0.0547, device='cuda:0')\n",
      "tensor(0.0273, device='cuda:0')\n",
      "tensor(0.5137, device='cuda:0')\n",
      "tensor(0.2568, device='cuda:0')\n",
      "tensor(0.1284, device='cuda:0')\n",
      "tensor(0.0642, device='cuda:0')\n",
      "tensor(0.0321, device='cuda:0')\n",
      "tensor(0.5161, device='cuda:0')\n",
      "tensor(0.7580, device='cuda:0')\n",
      "tensor(0.8790, device='cuda:0')\n",
      "tensor(0.4395, device='cuda:0')\n",
      "tensor(0.2198, device='cuda:0')\n",
      "tensor(0.1099, device='cuda:0')\n",
      "tensor(0.5549, device='cuda:0')\n",
      "tensor(0.7775, device='cuda:0')\n",
      "tensor(0.8887, device='cuda:0')\n",
      "tensor(0.4444, device='cuda:0')\n",
      "tensor(0.2222, device='cuda:0')\n",
      "tensor(0.6111, device='cuda:0')\n",
      "tensor(0.3055, device='cuda:0')\n",
      "tensor(0.1528, device='cuda:0')\n",
      "tensor(0.5764, device='cuda:0')\n",
      "tensor(0.7882, device='cuda:0')\n",
      "tensor(0.8941, device='cuda:0')\n",
      "tensor(0.4470, device='cuda:0')\n",
      "tensor(0.2235, device='cuda:0')\n",
      "tensor(0.1118, device='cuda:0')\n",
      "tensor(0.0559, device='cuda:0')\n",
      "tensor(0.0279, device='cuda:0')\n",
      "tensor(0.0140, device='cuda:0')\n",
      "tensor(0.0070, device='cuda:0')\n",
      "tensor(0.0035, device='cuda:0')\n",
      "tensor(0.5017, device='cuda:0')\n",
      "tensor(0.2509, device='cuda:0')\n",
      "tensor(0.1254, device='cuda:0')\n",
      "tensor(0.0627, device='cuda:0')\n",
      "tensor(0.0314, device='cuda:0')\n",
      "tensor(0.0157, device='cuda:0')\n",
      "tensor(0.0078, device='cuda:0')\n",
      "tensor(0.5039, device='cuda:0')\n",
      "tensor(0.2520, device='cuda:0')\n",
      "tensor(0.6260, device='cuda:0')\n",
      "tensor(0.8130, device='cuda:0')\n",
      "tensor(0.4065, device='cuda:0')\n",
      "tensor(0.7032, device='cuda:0')\n",
      "tensor(0.3516, device='cuda:0')\n",
      "tensor(0.6758, device='cuda:0')\n",
      "tensor(0.8379, device='cuda:0')\n",
      "tensor(0.4190, device='cuda:0')\n",
      "tensor(0.7095, device='cuda:0')\n",
      "tensor(0.8547, device='cuda:0')\n",
      "tensor(0.9274, device='cuda:0')\n",
      "tensor(0.4637, device='cuda:0')\n",
      "tensor(0.7318, device='cuda:0')\n",
      "tensor(0.3659, device='cuda:0')\n",
      "tensor(0.6830, device='cuda:0')\n",
      "tensor(0.8415, device='cuda:0')\n",
      "tensor(0.4207, device='cuda:0')\n",
      "tensor(0.2104, device='cuda:0')\n",
      "tensor(0.1052, device='cuda:0')\n",
      "tensor(0.0526, device='cuda:0')\n",
      "tensor(0.0263, device='cuda:0')\n",
      "tensor(0.0131, device='cuda:0')\n",
      "tensor(0.5066, device='cuda:0')\n",
      "tensor(0.2533, device='cuda:0')\n",
      "tensor(0.1266, device='cuda:0')\n",
      "tensor(0.0633, device='cuda:0')\n",
      "tensor(0.0317, device='cuda:0')\n",
      "tensor(0.5158, device='cuda:0')\n",
      "tensor(0.7579, device='cuda:0')\n",
      "tensor(0.8790, device='cuda:0')\n",
      "tensor(0.9395, device='cuda:0')\n",
      "tensor(0.9697, device='cuda:0')\n",
      "tensor(0.9849, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.4962, device='cuda:0')\n",
      "tensor(0.7481, device='cuda:0')\n",
      "tensor(0.8741, device='cuda:0')\n",
      "tensor(0.4370, device='cuda:0')\n",
      "tensor(0.7185, device='cuda:0')\n",
      "tensor(0.8593, device='cuda:0')\n",
      "tensor(0.4296, device='cuda:0')\n",
      "tensor(0.2148, device='cuda:0')\n",
      "tensor(0.6074, device='cuda:0')\n",
      "tensor(0.8037, device='cuda:0')\n",
      "tensor(0.9019, device='cuda:0')\n",
      "tensor(0.4509, device='cuda:0')\n",
      "tensor(0.2255, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2255, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn = update(qn ,a , rn)\n",
    "qn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77556e3e",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Agent Example\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>Q</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Q</mi><mi>n</mi></msub><mo>+</mo><mi>α</mi><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>n</mi></msub><mo>−</mo><msub><mi>Q</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Q_{n+1} = Q_n + \\alpha (R_n - Q_n) \n",
    "</annotation></semantics></math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca0da4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3b36d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = torch.tensor([0 ,1] , device= device )\n",
    "Q = torch.tensor([5, 5] , device=device , dtype=float)\n",
    "BATCH_SIZE = 100\n",
    "EPS = 0.1\n",
    "a = 0.5\n",
    "probabilities = torch.tensor([0.2, 0.8], device=device)  # 70% for action 0, 30% for action 1\n",
    "\n",
    "# Sample actions based on the defined probabilities\n",
    "actions_samples = torch.multinomial(probabilities, BATCH_SIZE, replacement=True )\n",
    "actions_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dfdc98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jamil\\AppData\\Local\\Temp\\ipykernel_12440\\363706602.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_samples = torch.tensor(rewards_samples , device=device ,dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_samples = torch.multinomial(probabilities, BATCH_SIZE, replacement=True )\n",
    "rewards_samples = torch.tensor(rewards_samples , device=device ,dtype=torch.float)\n",
    "rewards_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a4601d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 5.0 1.0  Action :  0\n",
      "3.0 2.5 0.0  Action :  1\n",
      "3.0 1.75 1.0  Action :  1\n",
      "3.0 1.375 1.0  Action :  1\n",
      "2.0 1.375 1.0  Action :  0\n",
      "2.0 1.1875 1.0  Action :  1\n",
      "1.5 1.1875 1.0  Action :  0\n",
      "1.25 1.1875 1.0  Action :  0\n",
      "1.25 1.09375 1.0  Action :  1\n",
      "1.25 1.046875 1.0  Action :  1\n",
      "1.25 1.0234375 1.0  Action :  1\n",
      "1.25 1.01171875 1.0  Action :  1\n",
      "1.125 1.01171875 1.0  Action :  0\n",
      "1.125 1.005859375 1.0  Action :  1\n",
      "1.0625 1.005859375 1.0  Action :  0\n",
      "1.0625 1.0029296875 1.0  Action :  1\n",
      "1.03125 1.0029296875 1.0  Action :  0\n",
      "1.015625 1.0029296875 1.0  Action :  0\n",
      "1.0078125 1.0029296875 1.0  Action :  0\n",
      "0.50390625 1.0029296875 0.0  Action :  0\n",
      "0.50390625 1.00146484375 1.0  Action :  1\n",
      "0.751953125 1.00146484375 1.0  Action :  0\n",
      "0.751953125 1.000732421875 1.0  Action :  1\n",
      "0.751953125 1.0003662109375 1.0  Action :  1\n",
      "0.751953125 1.00018310546875 1.0  Action :  1\n",
      "0.751953125 1.000091552734375 1.0  Action :  1\n",
      "0.751953125 1.0000457763671875 1.0  Action :  1\n",
      "0.751953125 1.0000228881835938 1.0  Action :  1\n",
      "0.751953125 0.5000114440917969 0.0  Action :  1\n",
      "0.751953125 0.25000572204589844 0.0  Action :  1\n",
      "0.751953125 0.6250028610229492 1.0  Action :  1\n",
      "0.751953125 0.8125014305114746 1.0  Action :  1\n",
      "0.751953125 0.9062507152557373 1.0  Action :  1\n",
      "0.8759765625 0.9062507152557373 1.0  Action :  0\n",
      "0.8759765625 0.9531253576278687 1.0  Action :  1\n",
      "0.43798828125 0.9531253576278687 0.0  Action :  0\n",
      "0.43798828125 0.4765626788139343 0.0  Action :  1\n",
      "0.718994140625 0.4765626788139343 1.0  Action :  0\n",
      "0.718994140625 0.7382813394069672 1.0  Action :  1\n",
      "0.718994140625 0.8691406697034836 1.0  Action :  1\n",
      "0.718994140625 0.4345703348517418 0.0  Action :  1\n",
      "0.3594970703125 0.4345703348517418 0.0  Action :  0\n",
      "0.3594970703125 0.7172851674258709 1.0  Action :  1\n",
      "0.3594970703125 0.8586425837129354 1.0  Action :  1\n",
      "0.3594970703125 0.4293212918564677 0.0  Action :  1\n",
      "0.3594970703125 0.7146606459282339 1.0  Action :  1\n",
      "0.3594970703125 0.8573303229641169 1.0  Action :  1\n",
      "0.67974853515625 0.8573303229641169 1.0  Action :  0\n",
      "0.67974853515625 0.9286651614820585 1.0  Action :  1\n",
      "0.67974853515625 0.46433258074102923 0.0  Action :  1\n",
      "0.67974853515625 0.7321662903705146 1.0  Action :  1\n",
      "0.67974853515625 0.8660831451852573 1.0  Action :  1\n",
      "0.339874267578125 0.8660831451852573 0.0  Action :  0\n",
      "0.339874267578125 0.9330415725926287 1.0  Action :  1\n",
      "0.339874267578125 0.9665207862963143 1.0  Action :  1\n",
      "0.6699371337890625 0.9665207862963143 1.0  Action :  0\n",
      "0.6699371337890625 0.48326039314815716 0.0  Action :  1\n",
      "0.8349685668945312 0.48326039314815716 1.0  Action :  0\n",
      "0.9174842834472656 0.48326039314815716 1.0  Action :  0\n",
      "0.4587421417236328 0.48326039314815716 0.0  Action :  0\n",
      "0.4587421417236328 0.24163019657407858 0.0  Action :  1\n",
      "0.7293710708618164 0.24163019657407858 1.0  Action :  0\n",
      "0.7293710708618164 0.12081509828703929 0.0  Action :  1\n",
      "0.7293710708618164 0.5604075491435196 1.0  Action :  1\n",
      "0.7293710708618164 0.7802037745717598 1.0  Action :  1\n",
      "0.7293710708618164 0.3901018872858799 0.0  Action :  1\n",
      "0.7293710708618164 0.69505094364294 1.0  Action :  1\n",
      "0.3646855354309082 0.69505094364294 0.0  Action :  0\n",
      "0.3646855354309082 0.84752547182147 1.0  Action :  1\n",
      "0.3646855354309082 0.923762735910735 1.0  Action :  1\n",
      "0.3646855354309082 0.9618813679553675 1.0  Action :  1\n",
      "0.3646855354309082 0.9809406839776837 1.0  Action :  1\n",
      "0.3646855354309082 0.9904703419888419 1.0  Action :  1\n",
      "0.3646855354309082 0.49523517099442094 0.0  Action :  1\n",
      "0.3646855354309082 0.7476175854972105 1.0  Action :  1\n",
      "0.3646855354309082 0.8738087927486052 1.0  Action :  1\n",
      "0.3646855354309082 0.9369043963743027 1.0  Action :  1\n",
      "0.3646855354309082 0.46845219818715134 0.0  Action :  1\n",
      "0.3646855354309082 0.7342260990935756 1.0  Action :  1\n",
      "0.3646855354309082 0.8671130495467878 1.0  Action :  1\n",
      "0.3646855354309082 0.9335565247733939 1.0  Action :  1\n",
      "0.3646855354309082 0.966778262386697 1.0  Action :  1\n",
      "0.3646855354309082 0.9833891311933485 1.0  Action :  1\n",
      "0.3646855354309082 0.9916945655966742 1.0  Action :  1\n",
      "0.3646855354309082 0.9958472827983371 1.0  Action :  1\n",
      "0.3646855354309082 0.9979236413991686 1.0  Action :  1\n",
      "0.3646855354309082 0.9989618206995843 1.0  Action :  1\n",
      "0.3646855354309082 0.9994809103497921 1.0  Action :  1\n",
      "0.3646855354309082 0.9997404551748961 1.0  Action :  1\n",
      "0.3646855354309082 0.999870227587448 1.0  Action :  1\n",
      "0.1823427677154541 0.999870227587448 0.0  Action :  0\n",
      "0.1823427677154541 0.999935113793724 1.0  Action :  1\n",
      "0.1823427677154541 0.499967556896862 0.0  Action :  1\n",
      "0.1823427677154541 0.249983778448431 0.0  Action :  1\n",
      "0.1823427677154541 0.6249918892242154 1.0  Action :  1\n",
      "0.591171383857727 0.6249918892242154 1.0  Action :  0\n",
      "0.591171383857727 0.8124959446121077 1.0  Action :  1\n",
      "0.591171383857727 0.9062479723060539 1.0  Action :  1\n",
      "0.2955856919288635 0.9062479723060539 0.0  Action :  0\n",
      "0.2955856919288635 0.953123986153027 1.0  Action :  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "\n",
    "    if random.random() > EPS :\n",
    "\n",
    "        reward = rewards_samples[i]\n",
    "        action = actions_samples[i]\n",
    "        Q[int(action)] = update(Q[int(action)] ,a , reward)\n",
    "\n",
    "    else :\n",
    "        action = torch.argmax(Q , dim=0)\n",
    "\n",
    "        if action == actions_samples[i]:\n",
    "            reward = rewards_samples[i]\n",
    "            Q[int(action)] = update(Q[int(action)] ,a , reward)\n",
    "        \n",
    "        else:\n",
    "            reward = torch.tensor(0 , device=device , dtype=torch.float) \n",
    "            Q[int(action)] = update(Q[int(action)] ,a , reward)\n",
    "    \n",
    "    print(Q[0].item() , Q[1].item() , reward.item() ,\" Action : \" ,action.item())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c56fa7",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48912914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent():\n",
    "\n",
    "        EVAL_TIME_STEPS = 100\n",
    "        TOTAL_REWARD = 0\n",
    "        N_EPISODES = 10\n",
    "        action = torch.argmax(Q , dim=0)\n",
    "        print(\"Chosen action is\" , int(action))\n",
    "\n",
    "        for i in range(N_EPISODES):\n",
    "                EPISODE_REWARD = 0\n",
    "                for i in range(EVAL_TIME_STEPS):\n",
    "\n",
    "                        action = torch.argmax(Q , dim=0)\n",
    "                        random_index_sample = random.randint(0 , len(actions_samples) - 1)\n",
    "\n",
    "                        if action == actions_samples[random_index_sample]:\n",
    "                                reward = rewards_samples[random_index_sample]\n",
    "                        else :\n",
    "                                reward = torch.tensor(0 , device=device , dtype=torch.float)\n",
    "                        \n",
    "                        EPISODE_REWARD+=reward\n",
    "                print(\"Episode Reward\" , EPISODE_REWARD)\n",
    "\n",
    "                TOTAL_REWARD+=EPISODE_REWARD\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Mean Reward: \", (TOTAL_REWARD / N_EPISODES).item()  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9548597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action is 1\n",
      "Episode Reward tensor(64., device='cuda:0')\n",
      "Episode Reward tensor(65., device='cuda:0')\n",
      "Episode Reward tensor(59., device='cuda:0')\n",
      "Episode Reward tensor(60., device='cuda:0')\n",
      "Episode Reward tensor(58., device='cuda:0')\n",
      "Episode Reward tensor(69., device='cuda:0')\n",
      "Episode Reward tensor(60., device='cuda:0')\n",
      "Episode Reward tensor(66., device='cuda:0')\n",
      "Episode Reward tensor(55., device='cuda:0')\n",
      "Episode Reward tensor(52., device='cuda:0')\n",
      "Mean Reward:  60.79999923706055\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14da37",
   "metadata": {},
   "source": [
    "### Upper confidence bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8679d99",
   "metadata": {},
   "source": [
    "### <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>a</mi></munder><mrow><mo fence=\"true\">(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>c</mi><msqrt><mfrac><mrow><mi>ln</mi><mo>⁡</mo><mi>t</mi></mrow><mrow><msub><mi>N</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></msqrt><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A_t = \\arg\\max_a \\left( Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right)\n",
    "</annotation></semantics></math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131489e",
   "metadata": {},
   "source": [
    "<math xmlns=“http://www.w3.org/1998/Math/MathML” display=“inline”><semantics><mrow><msub><mi>Q</mi><mi>t</mi></msub><mo stretchy=“false”>(</mo><mi>a</mi><mo stretchy=“false”>)</mo></mrow><annotation encoding=“application/x-tex”>Q_t(a)</annotation></semantics></math>: This is the estimated value of action ( a ) at time ( t ). It represents the average reward received from action ( a ) up to time ( t ).\n",
    "<math xmlns=“http://www.w3.org/1998/Math/MathML” display=“inline”><semantics><mrow><mi>c</mi></mrow><annotation encoding=“application/x-tex”>c</annotation></semantics></math>: This is a constant that controls the degree of exploration. A higher value of ( c ) encourages more exploration.\n",
    "<math xmlns=“http://www.w3.org/1998/Math/MathML” display=“inline”><semantics><mrow><msqrt><mfrac><mrow><mi>ln</mi><mo>⁡</mo><mi>t</mi></mrow><mrow><msub><mi>N</mi><mi>t</mi></msub><mo stretchy=“false”>(</mo><mi>a</mi><mo stretchy=“false”>)</mo></mrow></mfrac></msqrt></mrow><annotation encoding=“application/x-tex”>\\sqrt{\\frac{\\ln t}{N_t(a)}}</annotation></semantics></math>: This is the exploration term. It increases as the number of times action ( a ) has been selected, ( N_t(a) ), decreases. It also increases with the logarithm of the current time step ( t ), encouraging exploration of less frequently chosen actions.\n",
    "<math xmlns=“http://www.w3.org/1998/Math/MathML” display=“inline”><semantics><mrow><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>a</mi></munder></mrow><annotation encoding=“application/x-tex”>\\arg\\max_a</annotation></semantics></math>: This notation means that we select the action ( a ) that maximizes the entire expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e106c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 5.0 1.0  Action :  0\n",
      "3.8325546383857727 3.3325546383857727 0.0  Action :  1\n",
      "2.4403508603572845 4.380701720714569 0.0  Action :  0\n",
      "3.2252908647060394 3.2790558636188507 1.0  Action :  1\n",
      "4.071048349142075 2.062406674027443 0.0  Action :  1\n",
      "2.481712892651558 2.731689766049385 0.0  Action :  0\n",
      "3.1791923195123672 1.7145845964550972 0.0  Action :  1\n",
      "2.450102873146534 2.2913953140378 1.0  Action :  0\n",
      "1.5215122140944004 2.8843168690800667 0.0  Action :  0\n",
      "2.027321223169565 2.2456438578665257 1.0  Action :  1\n",
      "2.5434924997389317 1.880907567217946 1.0  Action :  1\n",
      "1.534472705796361 2.331295760348439 0.0  Action :  0\n",
      "1.992057265713811 1.3944401601329446 0.0  Action :  1\n",
      "1.2281025657430291 1.8005695501342416 0.0  Action :  0\n",
      "1.6395064303651452 1.1059867073781788 0.0  Action :  1\n",
      "1.0278918747790158 1.4760110010392964 0.0  Action :  0\n",
      "1.401939690578729 0.9250294084195048 0.0  Action :  1\n",
      "1.3898708822671324 1.265051286900416 1.0  Action :  0\n",
      "1.3665291221113876 1.6082386488560587 1.0  Action :  0\n",
      "1.6812233777018264 0.9772011694731191 0.0  Action :  1\n",
      "0.9992349669919349 1.2944477257551625 0.0  Action :  0\n",
      "1.2922576293931343 0.8070544181973673 0.0  Action :  1\n",
      "0.7936898730986286 1.1021765350014903 0.0  Action :  0\n",
      "1.0679528974287678 1.1996474056795705 1.0  Action :  1\n",
      "1.3439717553847004 1.2378331318177516 1.0  Action :  1\n",
      "0.8108335565921152 1.4956931069173152 0.0  Action :  0\n",
      "1.0701826941367472 1.3775211222309736 1.0  Action :  1\n",
      "1.3309587966796244 1.3104560721585585 1.0  Action :  1\n",
      "1.2965522026715917 1.5398334760138823 1.0  Action :  0\n",
      "1.5424500024018926 0.8851813325618423 0.0  Action :  1\n",
      "0.8947651418893656 1.1031933376525558 0.0  Action :  0\n",
      "1.1274712267368159 1.1611054141985733 1.0  Action :  1\n",
      "1.3612081143110117 1.1844357649320045 1.0  Action :  1\n",
      "1.2979703487380903 1.3821053157084862 1.0  Action :  0\n",
      "1.5198013128265302 1.2902928193327625 1.0  Action :  1\n",
      "0.8712546870820006 1.4795946677443226 0.0  Action :  0\n",
      "1.0823926375023802 0.8348094175217966 0.0  Action :  1\n",
      "1.1471544079836349 1.0164518604634623 1.0  Action :  0\n",
      "0.6743163254288902 1.1987416992543558 0.0  Action :  0\n",
      "0.866380878928112 1.190830161172073 1.0  Action :  1\n",
      "1.0590871956376304 0.6830088595538939 0.0  Action :  1\n",
      "0.6262088684453602 0.8511223743348921 0.0  Action :  0\n",
      "0.8109118608846302 1.0098821123435897 1.0  Action :  1\n",
      "0.9961784748448963 1.0859951997794113 1.0  Action :  1\n",
      "1.1819943903102512 0.6210402858752708 0.0  Action :  1\n",
      "0.6841729789085207 0.7715550226468544 0.0  Action :  0\n",
      "0.8625529169149218 0.9612459472069492 1.0  Action :  1\n",
      "1.0414198993749437 0.553494710785615 0.0  Action :  1\n",
      "0.6103813007030396 0.6944068412606592 0.0  Action :  0\n",
      "0.782371173754683 0.41784211449944353 0.0  Action :  1\n",
      "0.47739790154265904 0.5545926784785924 0.0  Action :  0\n",
      "0.6430456913785889 0.8458402576701014 1.0  Action :  1\n",
      "0.8090922809915497 0.4893387617000028 0.0  Action :  1\n",
      "0.4877646479644753 0.618193215263693 0.0  Action :  0\n",
      "0.6479112420120217 0.87367184764783 1.0  Action :  1\n",
      "0.8084174755849816 0.49953366968353796 0.0  Action :  1\n",
      "0.48463809806299096 0.6213963419528823 0.0  Action :  0\n",
      "0.6396422566790569 0.37176041381220126 0.0  Action :  1\n",
      "0.8974861816475386 0.490542263554137 1.0  Action :  0\n",
      "0.523685634926438 0.609568656252425 0.0  Action :  0\n",
      "0.668509141400933 0.3644175347023214 0.0  Action :  1\n",
      "0.9068093899863361 0.4805052470498289 1.0  Action :  0\n",
      "0.5235932843872606 0.5968177657656873 0.0  Action :  0\n",
      "0.6595488863178789 0.8566755710210903 1.0  Action :  1\n",
      "0.7957576738783418 0.9850914469940714 1.0  Action :  1\n",
      "0.9322153257319032 0.5478663924626137 0.0  Action :  1\n",
      "0.5344588198755532 0.6557892743042733 0.0  Action :  0\n",
      "0.6669842490051537 0.3819510617997063 0.0  Action :  1\n",
      "0.39986936194173245 0.48747385086077577 0.0  Action :  0\n",
      "0.5286935959505215 0.7965878945661491 1.0  Action :  1\n",
      "0.657732719604887 0.9499095952547046 1.0  Action :  1\n",
      "0.786983350341238 1.0253940697032438 1.0  Action :  1\n",
      "0.9164422546075954 1.0620147114256135 1.0  Action :  1\n",
      "1.046106258456625 0.5792544285585042 0.0  Action :  1\n",
      "0.5879861461245603 0.6737024511028882 0.0  Action :  0\n",
      "0.7141098575913496 0.8841476192141352 1.0  Action :  1\n",
      "0.8404237823807783 0.9883889171016602 1.0  Action :  1\n",
      "0.9669251786791868 0.5395699621504738 0.0  Action :  1\n",
      "0.5468056970399413 0.6285198585456803 0.0  Action :  0\n",
      "0.6699427091640029 0.8587988481554485 1.0  Action :  1\n",
      "0.7932541319292579 0.9730722204165209 1.0  Action :  1\n",
      "0.9167375811499152 1.029377306268441 1.0  Action :  1\n",
      "1.0403907396954093 1.0567307259966072 1.0  Action :  1\n",
      "0.5821056750199096 1.139277802043074 0.0  Action :  0\n",
      "0.7025490861913459 1.1109675211871235 1.0  Action :  1\n",
      "0.8231509414097564 1.0960709250551743 1.0  Action :  1\n",
      "0.9439091992160575 1.0879084739378235 1.0  Action :  1\n",
      "1.06482187030523 1.0831389004136693 1.0  Action :  1\n",
      "0.5929435124884495 1.1601803624774587 0.0  Action :  0\n",
      "0.7107921472082981 1.1186588269544429 1.0  Action :  1\n",
      "0.828785397697533 0.5972558154201898 0.0  Action :  1\n",
      "0.4734607490842764 0.6718680901861581 0.0  Action :  0\n",
      "0.5885412993972723 0.3732847481239037 0.0  Action :  1\n",
      "0.3518787772926303 0.44678477041604087 0.0  Action :  0\n",
      "0.4641935901316615 0.7601851701372959 1.0  Action :  1\n",
      "0.5766374619277926 0.4163033231911112 0.0  Action :  1\n",
      "0.34460445128245215 0.48759856967300663 0.0  Action :  0\n",
      "0.454412337465714 0.7794868469147409 1.0  Action :  1\n",
      "0.5643417277173506 0.4248847871973378 0.0  Action :  1\n",
      "0.33719563301584654 0.4941094949796063 0.0  Action :  0\n"
     ]
    }
   ],
   "source": [
    "c = 2\n",
    "Q = torch.tensor([5, 5] , device=device , dtype=float)\n",
    "i =0\n",
    "\n",
    "actions_freq = torch.ones((2,) , device=device , dtype=torch.float)\n",
    "\n",
    "def upper_confidence_bound(Q , c , t , nt_a):\n",
    "\n",
    "    for i in range(len(Q)):\n",
    "          \n",
    "          Q[i] = Q[i] + c * (   torch.sqrt( torch.log(t)  ) /  nt_a[i]      )\n",
    "    return Q\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "\n",
    "    \n",
    "    action = torch.argmax( upper_confidence_bound(Q , c ,torch.tensor(i+1 , device=device) , actions_freq) , dim=0)\n",
    "\n",
    "    if action == actions[0]:\n",
    "\n",
    "        actions_freq[0]+= torch.tensor(1 , device=device , dtype=torch.float)\n",
    "    else :\n",
    "        actions_freq[1]+= torch.tensor(1 , device=device , dtype=torch.float)       \n",
    "\n",
    "    if action == actions_samples[i]:\n",
    "            reward = rewards_samples[i]\n",
    "            Q[int(action)] = update(Q[int(action)] ,a , reward)\n",
    "        \n",
    "    else:\n",
    "            reward = torch.tensor(0 , device=device , dtype=torch.float) \n",
    "            Q[int(action)] = update(Q[int(action)] ,a , reward)\n",
    "    \n",
    "    print(Q[0].item() , Q[1].item() , reward.item() ,\" Action : \" ,action.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "709372b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action is 1\n",
      "Episode Reward tensor(69., device='cuda:0')\n",
      "Episode Reward tensor(65., device='cuda:0')\n",
      "Episode Reward tensor(58., device='cuda:0')\n",
      "Episode Reward tensor(65., device='cuda:0')\n",
      "Episode Reward tensor(58., device='cuda:0')\n",
      "Episode Reward tensor(63., device='cuda:0')\n",
      "Episode Reward tensor(58., device='cuda:0')\n",
      "Episode Reward tensor(60., device='cuda:0')\n",
      "Episode Reward tensor(60., device='cuda:0')\n",
      "Episode Reward tensor(57., device='cuda:0')\n",
      "Mean Reward:  61.29999923706055\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0a6aa",
   "metadata": {},
   "source": [
    "# Decaying Past Rewards\n",
    "\n",
    "$$\n",
    "Q = (1 - \\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha(1 - \\alpha)^{n-i} R_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbf487",
   "metadata": {},
   "source": [
    "# Expected returns\n",
    " The Expected retruns over the episode is used for the agent to maximize, the expected concept is used since the state rewards must be represented in probabilities and are not certain\n",
    " # <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"double-struck\">E</mi><mo stretchy=\"false\">[</mo><mi>R</mi><mo stretchy=\"false\">]</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>p</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>r</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{E}[R] = \\sum_{i=1}^{n} p_i \\cdot r_i\n",
    "</annotation></semantics></math>\n",
    "\n",
    "# <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"double-struck\">E</mi><mo stretchy=\"false\">[</mo><mi>R</mi><mo stretchy=\"false\">]</mo><mo>=</mo><msubsup><mo>∫</mo><mi>a</mi><mi>b</mi></msubsup><mi>r</mi><mo>⋅</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mo stretchy=\"false\">)</mo><mtext> </mtext><mi>d</mi><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{E}[R] = \\int_{a}^{b} r \\cdot p(r) \\, dr\n",
    "</annotation></semantics></math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06bbe13",
   "metadata": {},
   "source": [
    "# Reward Recursive Equation\n",
    "\n",
    "# <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>G</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">G_t = R_{t+1} + \\gamma G_{t+1}\n",
    "</annotation></semantics></math>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mtable rowspacing=\"0.25em\" columnalign=\"right left\" columnspacing=\"0em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mo stretchy=\"false\">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>:</mo><mtext>Expected&nbsp;return&nbsp;at&nbsp;time&nbsp;</mtext><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>:</mo><mtext>Reward&nbsp;received&nbsp;at&nbsp;time&nbsp;</mtext><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mo stretchy=\"false\">(</mo><mi>γ</mi><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>:</mo><mtext>Discount&nbsp;factor,&nbsp;</mtext><mo stretchy=\"false\">(</mo><mn>0</mn><mo>≤</mo><mi>γ</mi><mo>≤</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mo stretchy=\"false\">(</mo><msub><mi>G</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>:</mo><mtext>Expected&nbsp;return&nbsp;at&nbsp;time&nbsp;</mtext><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{align*}\n",
    "( G_t ) &amp;: \\text{Expected return at time } ( t ). \\\\\n",
    "( R_{t+1} ) &amp;: \\text{Reward received at time } ( t+1 ). \\\\\n",
    "( \\gamma ) &amp;: \\text{Discount factor, } ( 0 \\leq \\gamma \\leq 1 ). \\\\\n",
    "( G_{t+1} ) &amp;: \\text{Expected return at time } ( t+1 ).\n",
    "\\end{align*}\n",
    "</annotation></semantics></math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472a350",
   "metadata": {},
   "source": [
    "#### The second term is the function calling itself on time step t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd0d2ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3616, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor([1,1,1,1,1] , device=device)\n",
    "y = torch.tensor(0.8 , device=device)\n",
    "\n",
    "def calculate_reward(rt , y):\n",
    "\n",
    "    gt = 0\n",
    "    t = 0\n",
    "    \n",
    "    for reward in rt:\n",
    "         gt+= y **t * reward\n",
    "         t+=1\n",
    "    \n",
    "    return gt\n",
    "\n",
    " \n",
    "    \n",
    "gt = calculate_reward(rewards , y )\n",
    "\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1391e43",
   "metadata": {},
   "source": [
    "# Note : The number of possible determinstic policies are equal to the number of actions ^ number of possible states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989e2e4",
   "metadata": {},
   "source": [
    "# Determenstic value function update bellman equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e477a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TabularDiscrete import TabularEnv\n",
    "env = TabularEnv()\n",
    "state , _ = env.reset()\n",
    "\n",
    "P = torch.tensor( [0.5 , 0.5] , device=device , dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55546d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6044, 0.2589], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.tensor(env.table.values , device=device , dtype=torch.float)[: , 2]\n",
    "V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60c3f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6044, 0.2589], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = V.clone()\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86ab7ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8462, 0.3624], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_value_determenstic(R , gamma , V ):\n",
    "    return R + gamma * (P * V)\n",
    "\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "V_prime =update_value_determenstic(R , gamma , V)\n",
    "V_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d58c93",
   "metadata": {},
   "source": [
    "# R + γ P max_a’ Q\n",
    "\n",
    "### ( R ) is a matrix of expected rewards for all state-action pairs.\n",
    "( \\gamma ) is the discount factor.\n",
    "( P ) is the transition probability matrix.\n",
    "( \\max_{a’} Q ) is a vector where each element represents the maximum action-value for the next state ( s’ ) over all possible actions ( a’ ).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
